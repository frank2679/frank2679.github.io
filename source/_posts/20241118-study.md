---
title: study notes
date: 2024-11-18 13:46:02
tags: [ML, LLM]
---

# 概览


## 目标

能够部署 E2E 业务，提升自己的开发效率，以及做一个产品实现商业闭环。

## 资源

- [头歌实践教学平台](https://www.educoder.net/)
- [ustc, zhangyu, 编译原理课程 resource](http://staff.ustc.edu.cn/~yuzhang/compiler/2023f/nsec/index.html)

大佬
- [Dennis Ritchie](https://www.bell-labs.com/usr/dmr/www/)
- [Brian Kernighan](https://www.cs.princeton.edu/~bwk/)

# vLLM

[what is vLLM?](https://www.hopsworks.ai/dictionary/vllm)
[Qwen doc, 2.5 都被 vLLM 支持](https://qwen.readthedocs.io/zh-cn/latest/deployment/vllm.html)

vLLM 的优点
- 易于使用
- 具有最先进的服务吞吐量
- 高效 kv cache 内存管理（通过 pagedAttention 实现）
- 连续批处理输入请求
- 优化的 CUDA 内核

核心概念
- 词汇表总规模 151665

# 分布式

## 概览
并行算法
- 模型并行
  - 训练精度等同单卡
  - 切分模型，不同机器跑不同部分
  - 要求设备间数据传输快
  - 典型场景：单机多卡
- 数据并行
  - 训练精度低于单卡
  - 切分batch
  - 每个 device 存放并运行完整模型
  - 集群主流算法
![](model_paral.png)

![](data_paral.png)

数据并行 all reduce 算法
- NVIDIA
  - NCCL 2.3 hierarchical ring allreduce
  - NCCL 2.4 double tree all reduce
- TPU
  - 2D torus ring

## 消息通信
MPI：起源于科学计算
NCCL(NVIDIA Collective Communications Library) API 与 MPI 类似

操作包括
- all reduce
- reduce
- broadcast
- allgather
- reducescatter

all reduce
ring all reduce
- 每个卡上数据切块，数量与卡数量相等
- 每次 loop 每个卡只算某个分块求和，loop 多次后每个卡上最终求和的部分分块

特征
- 扩展性较差
  - 分块受 GPU 数量影响，数量增加延时增加

扩展性问题解决——结构化 ring
- 2D torus TPU v3，3步
  - ![alt text](torus_alg.png)
- hyrid ring, NCCL 2.3
  - 2步，![alt text](hibrid_alg.png)
  - 3步，![alt text](binary_tree_alg.png)


# pytorch

JIT，torchscript

- [知乎, PyTorch系列「一」PyTorch JIT —— trace/ script的代码组织和优化方法](https://zhuanlan.zhihu.com/p/410507557)
- [github, JIT overview](https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md)
- [pytorch, torchscript](https://pytorch.org/docs/main/jit.html)
- [lecture, 面向机器学习的编译](http://staff.ustc.edu.cn/~yuzhang/compiler/2019f/lectures/c4ml-6in1.pdf)

eager mode 易于调试，但是不利于性能优化，JIT 模式将原本 eager mode 表达转变成一个计算图，进行优化和序列化。（向TF靠拢）

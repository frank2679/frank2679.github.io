---
title: study notes
date: 2024-11-18 13:46:02
tags: [ML, LLM]
---

- [1. 概览](#1-概览)
  - [1.1. 目标](#11-目标)
  - [1.2. 资源](#12-资源)
    - [1.2.1. 平台](#121-平台)
    - [1.2.2. 大佬 blog](#122-大佬-blog)
- [2. 如何参与开源项目](#2-如何参与开源项目)
- [3. vLLM](#3-vllm)
- [4. 分布式](#4-分布式)
  - [4.1. 概览](#41-概览)
  - [4.2. 消息通信](#42-消息通信)
- [5. pytorch](#5-pytorch)
- [6. LLVM](#6-llvm)
- [7. cuda / supa](#7-cuda--supa)
  - [7.1. preamble kernel](#71-preamble-kernel)
- [8. 性能调优](#8-性能调优)
  - [8.1. 理论性能分析](#81-理论性能分析)
  - [8.2. 优化策略](#82-优化策略)
  - [8.3. 性能优化工具](#83-性能优化工具)
- [log](#log)
  - [11.25](#1125)


# 1. 概览


## 1.1. 目标

能够部署 E2E 业务，提升自己的开发效率，以及做一个产品实现商业闭环。

## 1.2. 资源

### 1.2.1. 平台
- [头歌实践教学平台](https://www.educoder.net/)
- [ustc, zhangyu, 编译原理课程 resource](http://staff.ustc.edu.cn/~yuzhang/compiler/2023f/nsec/index.html)

### 1.2.2. 大佬 blog
- [Dennis Ritchie](https://www.bell-labs.com/usr/dmr/www/)
- [Brian Kernighan](https://www.cs.princeton.edu/~bwk/)
- [Stephen Wolfram](https://www.stephenwolfram.com/)
- [美团技术团队](https://tech.meituan.com/)
- [阿里正一，那些“混得不好”的人，大都没什么人格魅力](https://ilinuxkernel.com/?p=2180)
- [御坂研究所，编译原理](https://www.nosuchfield.com/2017/07/16/Talk-about-compilation-principles-1/)



# 2. 如何参与开源项目

- [如何参与 Apache 顶级开源项目](https://www.nosuchfield.com/2024/01/24/How-to-Participate-in-Apache-Top-Level-Projects/)

1. 找一个感兴趣的项目，提交第一个 PR
2. 修复 issue 中的问题，提交 PR

- [小宇宙，S04E00 - 吴晟：开源项目进入 Apache 孵化器意味着什么](https://www.xiaoyuzhoufm.com/episode/651fb7082e5a4700d96ccc71)


# 3. vLLM

[what is vLLM?](https://www.hopsworks.ai/dictionary/vllm)
[Qwen doc, 2.5 都被 vLLM 支持](https://qwen.readthedocs.io/zh-cn/latest/deployment/vllm.html)

vLLM 的优点
- 易于使用
- 具有最先进的服务吞吐量
- 高效 kv cache 内存管理（通过 pagedAttention 实现）
- 连续批处理输入请求
- 优化的 CUDA 内核

核心概念
- 词汇表总规模 151665

# 4. 分布式

## 4.1. 概览
并行算法
- 模型并行
  - 训练精度等同单卡
  - 切分模型，不同机器跑不同部分
  - 要求设备间数据传输快
  - 典型场景：单机多卡
- 数据并行
  - 训练精度低于单卡
  - 切分batch
  - 每个 device 存放并运行完整模型
  - 集群主流算法
![](model_paral.png)

![](data_paral.png)

数据并行 all reduce 算法
- NVIDIA
  - NCCL 2.3 hierarchical ring allreduce
  - NCCL 2.4 double tree all reduce
- TPU
  - 2D torus ring

## 4.2. 消息通信
MPI：起源于科学计算
NCCL(NVIDIA Collective Communications Library) API 与 MPI 类似

操作包括
- all reduce
- reduce
- broadcast
- allgather
- reducescatter

all reduce
ring all reduce
- 每个卡上数据切块，数量与卡数量相等
- 每次 loop 每个卡只算某个分块求和，loop 多次后每个卡上最终求和的部分分块

特征
- 扩展性较差
  - 分块受 GPU 数量影响，数量增加延时增加

扩展性问题解决——结构化 ring
- 2D torus TPU v3，3步
  - ![alt text](torus_alg.png)
- hyrid ring, NCCL 2.3
  - 2步，![alt text](hibrid_alg.png)
  - 3步，![alt text](binary_tree_alg.png)


# 5. pytorch

JIT，torchscript

- [知乎, PyTorch系列「一」PyTorch JIT —— trace/ script的代码组织和优化方法](https://zhuanlan.zhihu.com/p/410507557)
- [github, JIT overview](https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md)
- [pytorch, torchscript](https://pytorch.org/docs/main/jit.html)
- [lecture, 面向机器学习的编译](http://staff.ustc.edu.cn/~yuzhang/compiler/2019f/lectures/c4ml-6in1.pdf)

eager mode 易于调试，但是不利于性能优化，JIT 模式将原本 eager mode 表达转变成一个计算图，进行优化和序列化。（向TF靠拢）


# 6. LLVM
- [电子书 getting-started-with-llvm-core-libraries.pdf, github有其他资源](https://github.com/firmianay/security-paper/blob/master/Compiler/Getting_Started_with_LLVM_Core_Libraries/Getting%20Started%20with%20LLVM%20Core%20Libraries.pdf)
- [中文版，getting-started-with-llvm-core-libraries.pdf](https://getting-started-with-llvm-core-libraries-zh-cn.readthedocs.io/zh-cn/latest/)

代码 https://github.com/llvm/llvm-project/tree/release/10.x
 
官方文档汇总 https://llvm.org/docs/
官方培训材料 http://llvm.org/docs/tutorial/index.html
 
编程规范 https://llvm.org/docs/CodingStandards.html
编程指南 https://llvm.org/docs/ProgrammersManual.html 
 
后端编译器实现 http://llvm.org/docs/WritingAnLLVMBackend.html  
通用代码生成 http://llvm.org/docs/CodeGenerator.html
自定义扩展 https://llvm.org/docs/ExtendingLLVM.html
 
TableGen简介 http://llvm.org/docs/TableGen/index.html
FileCheck简介 https://llvm.org/docs/CommandGuide/FileCheck.html  
 
LLVM IR http://llvm.org/docs/LangRef.html
Writing a Pass http://llvm.org/docs/WritingAnLLVMPass.html 

Life of an instruction in LLVM https://eli.thegreenplace.net/2012/11/24/life-of-an-instruction-in-llvm/

# 7. cuda / supa

## 7.1. preamble kernel

- [cuda doc,  Programmatic Dependent Launch and Synchronization](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programmatic-dependent-launch-and-synchronization)

cuda 12 引入的 feature，目的是优化前一个 kernel launch 最后一个 wave 用不满的问题，可以让后面的 kernel 提前执行，提前的部分就叫做 preamble。



# 8. 性能调优

## 8.1. 理论性能分析

## 8.2. 优化策略

## 8.3. 性能优化工具

# log

## 11.25

1. 磨刀，走得远
   1. 系统学习，通过课程学习
   2. 看大佬们在做什么，读论文，读博客，听访谈
2. 砍柴，走得快
   1. 做项目
